---
title: "clustering_prep"
author: "LT"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(factoextra, tidyverse, cluster, stats)
```

# clustering

- standardization is necessary: de-unit is mandatory for clustering so we are comparing apple to apple.

```{r}
# data
data("USArrests")  # Load the data set
df <- USArrests    # Use df as shorter name
df |> head(5)
# remove NA
df <- na.omit(df)
# preprocess
df <- scale(df)
```
## k-means clustering
```{r}
require(stats) # for kmeans()
require(factoextra) # for vizing clustering results

# how to choose the number of clusters (K)?
fviz_nbclust(df, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)

# Compute k-means with k = 4
set.seed(1)
km.res <- kmeans(df, centers = 4, nstart = 25)

# viz kmeans clusters
fviz_cluster(km.res, data = df,
            palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
            ellipse.type = "euclid", # Concentration ellipse
            star.plot = F, # Add segments from centroid to items
            repel = TRUE, # Avoid label overplotting (slow)
            ggtheme = theme_minimal()
)
```
- observe for the "elbow"
- Cons
  - requires prior knowledge of the data and prespecified K
  - sensitive to outliers
  - high variances to K
  - ordering rows changes outcomes

## PAM 
- alternative to k-means that is less sensitive to outliers.

```{r}
require(cluster) # for pam()

# determine K
fviz_nbclust(df, pam, method = "silhouette")+
  theme_classic()

# pam
pam.res <- pam(df, 2)

# viz
fviz_cluster(pam.res,
             geom = c("point", "text"),
             palette = c("#00AFBB", "#FC4E07"), # color palette
             ellipse.type = "t", # Concentration ellipse
             repel = TRUE, # Avoid label overplotting (slow)
             ggtheme = theme_classic()
             )
```

## Hierarchical Clustering
- Complete linkage and Ward’s minimal inter-cluster variance method are generally preferred. 
- The higher the height of the fusion, the less similar the objects are.
- remember to verify clustering results by correlating the cophenetic distance to original distance
```{r}
# Compute the dissimilarity matrix
res.dist <- dist(df, method = "euclidean") 
# linkage: take distance matrix and compute dissimilarity among groups
res.hc <- hclust(d = res.dist, method = "ward.D2")

# viz dendrogram
fviz_dend(res.hc, cex = .5)

# Cut in 4 groups and color by groups
fviz_dend(res.hc, k = 4, # Cut in 4 groups
          cex = 0.5, # label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE # Add rectangle around groups
          )
```

```{r}
# horizontal dendrogram
fviz_dend(res.hc, k = 4, 
          cex = 0.4, 
          horiz = TRUE, 
          k_colors = "jco",
          rect = TRUE, 
          rect_border = "jco", 
          rect_fill = TRUE)

# circular/phylogenic
fviz_dend(res.hc, k = 4, 
          cex = 0.4, 
          horiz = TRUE, 
          k_colors = "jco",
          rect = TRUE, 
          rect_border = "jco", 
          rect_fill = TRUE,
          type = "circular")
```
```{r}
# Create a plot of the whole dendrogram,
# and extract the dendrogram data
dend_plot <- fviz_dend(res.hc, k = 4, # Cut in four groups
                       cex = 0.5, # label size
                       k_colors = "jco"
                       )
dend_data <- attr(dend_plot, "dendrogram") # Extract dendrogram data
# Cut the dendrogram at height h = 10
dend_cuts <- cut(dend_data, h = 10)
# Visualize the truncated version containing
# two branches
fviz_dend(dend_cuts$upper)
# plot whole dendrogram
print(dend_plot)
# Plot subtree 1
fviz_dend(dend_cuts$lower[[1]], main = "Subtree 1")
# Plot subtree 2
fviz_dend(dend_cuts$lower[[2]], main = "Subtree 2", 
          type = "circular",  
          repel = TRUE)
```
```{r}
# customization with dendextend
require(dendextend)
# 1. Create a customized dendrogram
mycols <- c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07")
dend <- as.dendrogram(res.hc) %>%
    set("branches_lwd", 1) %>% # Branches line width
    set("branches_k_color", mycols, k = 4) %>% # Color branches by groups
    set("labels_colors", mycols, k = 4) %>% # Color labels by groups
    set("labels_cex", 0.5) # Change label size
# 2. Create plot
fviz_dend(dend)
```

```{r}
# save dendrogram
pdf("dendrogram.pdf", width=30, height=15) # Open a PDF
p <- fviz_dend(res.hc, k = 4, cex = 1, k_colors = "jco" ) # Do plotting
print(p)
dev.off()
```

## heatmap
- another way to visualize hierarchical clustering where data values are converted to color scale.

```{r}
require(stats) # heatmap()
require(gplots) # heatmap.2() enhanced heatmap
require(pheatmap) # Draws pretty heatmaps and provides more control to change the appearance
require(d3heamap) # interactive heatmap
require(ComplexHeatmap) # Draws, annotates and arranges complex heatmaps (very useful for genomic)
```

```{r}
df <- scale(mtcars)
# base
heatmap(df, scale = "none")
# Use RColorBrewer color palette names
require("RColorBrewer")
col <- colorRampPalette(brewer.pal(10, "RdYlBu"))(256)
heatmap(df, scale = "none", col = col,
        RowSideColors = rep(c("blue", "pink"), each = 16),
        ColSideColors = c(rep("purple", 5), rep("orange", 6)))
```
```{r}
# enhanced version
require(ComplexHeatmap)
Heatmap(df, 
        name = "mtcars", # title of legend
        column_title = "Variables", 
        row_title = "Samples",
        show_column_names = T,
        show_row_names = T,
        show_column_dend = T,
        show_row_dend = T,
         clustering_distance_rows = "euclidean",
        clustering_method_rows = "ward.D2",
        row_names_gp = gpar(fontsize = 7)) # Text size for row names)

# Splitting heatmap by rows
Heatmap(df, 
        split = mtcars$cyl, 
        name = "mtcars", # title of legend
        row_names_gp = gpar(fontsize = 7))

# Split by combining multiple variables
Heatmap(df, name ="mtcars",
        split = data.frame(cyl = mtcars$cyl, am = mtcars$am))

############################################################
# annotation
############################################################
# Annotation data frame
annot_df <- data.frame(cyl = mtcars$cyl, 
                       am = mtcars$am,
                       mpg = mtcars$mpg)
# Define colors for each levels of qualitative variables
# Define gradient color for continuous variable (mpg)
col = list(cyl = c("4" = "green", "6" = "gray", "8" = "darkred"),
           am = c("0" = "yellow", "1" = "orange"),
           mpg = circlize::colorRamp2(c(17, 25),
                                      c("lightblue", "purple")) 
           )
# Create the heatmap annotation
ha <- HeatmapAnnotation(df = annot_df, col = col, 
                        show_legend = F)
# Combine the heatmap and the annotation
Heatmap(t(df), name = "mtcars",
        top_annotation = ha)
```

```{r}
# complex Annotation 
df <- t(df)
# Define some graphics to display the distribution of columns
.hist = anno_histogram(df, 
                       gp = gpar(fill = "lightblue"))
.density = anno_density(df, type = "line", 
                        gp = gpar(col = "blue"))

ha_mix_top = HeatmapAnnotation(hist = .hist, 
                               density = .density, 
                               which = "column",
                               height = unit(3, "cm"))
# Define some graphics to display the distribution of rows
.violin = anno_density(df, type = "violin",
                       gp = gpar(fill = "lightblue"), 
                       which = "row")
.boxplot = anno_boxplot(df, which = "row")

ha_mix_right = HeatmapAnnotation(violin = .violin, 
                                 bxplt = .boxplot,
                                 which = "row", 
                                 width = unit(4, "cm"))
# Combine annotation with heatmap
Heatmap(df, name = "mtcars",
        column_names_gp = gpar(fontsize = 8),
        top_annotation = ha_mix_top) + 
  ha_mix_right
```

# dimenson reduction: maximizing similarity
- t-SNE is one of the most popular non-linear dimension-reduction algorithms, as compared to PCA which is linear. It measures the distance between each observation in the data set and every other observation, and then randomizes the observations across (usually) two new axes. The observations are then iteratively shuffled around these new axes until their distances to each other in this two-dimensional space are as similar to the distances in the original high-dimensional space as possible.
  - outperforms PCA
  - It is infamously computationally expensive
  - It cannot project new data onto the embedding
  - Distances between clusters often don’t mean anything
  - We need to select sensible values for a number of hyperparameters

- UMAP is another nonlinear dimension-reduction algorithm that improves upon t-SNE. It works similarly to t-SNE (finds distances in a feature space with many variables and then tries to reproduce these distances in low-dimensional space), but differs in the way it measures distances.
  - faster than t-sne
  - a deterministic algorithm
  - UMAP preserves both local and global structure: not only can we interpret two cases close to each other in lower dimensions as being similar to each other in high dimensions, but we can also interpret two clusters of cases close to each other as being more similar to each other in high dimensions.
  
- overall
  - They can learn nonlinear patterns in the data.
  - They tend to separate clusters of cases better than PCA.
  - UMAP can make predictions on new data.
  - UMAP is computationally inexpensive.
  - UMAP preserves both local and global distances.
- but
  -  The new axes of t-SNE and UMAP are not directly
  interpretable in terms of the original variables.
  - t-SNE cannot make predictions on new data (different result each time).
  - t-SNE is computationally expensive.
  - t-SNE doesn’t necessarily preserve global structure.
  - They cannot handle categorical variables natively.
  
## t-sne
```{r}
require(Rtsne)
require(tidyverse)

# LOADING DATA ----
data(banknote, package = "mclust")
swissTib <- as_tibble(banknote)
swissTib
```
```{r}
# CREATE GRID OF t-SNE HYPERPARAMETERS ----
tsneHyperPars <- expand.grid(perplexity = c(1, 10, 30, 40, 50), 
                             theta      = seq(0.0, 1.0, 0.25),
                             eta        = c(1, 100, 200, 300, 400),
                             max_iter   = c(100, 300, 500, 700, 1000))

tsne <- pmap(tsneHyperPars, Rtsne, X = swissTib[, -1], verbose = TRUE)

tsneTib <- tibble(perplexity = rep(tsneHyperPars$perplexity, each = 200),
                  theta      = rep(tsneHyperPars$theta, each = 200),
                  eta        = rep(tsneHyperPars$eta, each = 200),
                  max_iter   = rep(tsneHyperPars$max_iter, each = 200),
                  tSNE1      = unlist(map(tsne, ~.$Y[, 1])),
                  tSNE2      = unlist(map(tsne, ~.$Y[, 2])))

klTib <- mutate(tsneHyperPars, 
                KL = map(tsne, ~round(tail(.$itercosts, 1), 3)))

filter(tsneTib, eta == 200, max_iter == 1000) %>%
  ggplot(aes(tSNE1, tSNE2)) +
  facet_grid(theta ~ perplexity) +
  geom_text(data = filter(klTib, eta == 200, max_iter == 1000), 
            aes(label = KL), x = -80, y = -80) +
  geom_point() +
  theme_bw()
```
```{r}
filter(tsneTib, perplexity == 30, theta == 0.5) %>%
  ggplot(aes(tSNE1, tSNE2)) +
  facet_grid(max_iter ~ eta) +
  geom_text(data = filter(klTib, perplexity == 30, theta == 0.5),
            aes(label = KL), x = -50, y = -40) +
  geom_point() +
  theme_bw()

# FINAL t-SNE ----
swissTsne <- select(swissTib, -Status) %>%
  Rtsne(perplexity = 30, theta = 0, max_iter = 5000, verbose = TRUE)

swissTibTsne <- swissTib %>%
  mutate_if(.funs = scale, .predicate = is.numeric, scale = FALSE) %>% # center variables
  mutate(tSNE1 = swissTsne$Y[, 1], tSNE2 = swissTsne$Y[, 2]) %>%
  gather(key = "Variable", value = "Value", c(-tSNE1, -tSNE2, -Status))

ggplot(swissTibTsne, aes(tSNE1, tSNE2, col = Value, shape = Status)) +
  facet_wrap(~ Variable) +
  geom_point(size = 3) +
  scale_color_gradient(low = "dark blue", high = "cyan") +
  theme_bw()
```


## umap
```{r}
require(umap)
# CREATE GRID OF UMAP HYPERPARAMETERS ----
umapHyperPars <- expand.grid(n_neighbors = seq(3, 19, 4),
                             min_dist    = seq(0.1, 0.5, 0.1),
                             metric      = c("euclidean", "manhattan"),
                             n_epochs    = seq(50, 400, 75))

umap <- pmap(umapHyperPars, umap, d = swissTib[, -1], verbose = TRUE)

umapTib <- tibble(n_neighbors = rep(umapHyperPars$n_neighbors, each = 200),
                  min_dist      = rep(umapHyperPars$min_dist, each = 200),
                  metric        = rep(umapHyperPars$metric, each = 200),
                  n_epochs   = rep(umapHyperPars$n_epochs, each = 200),
                  UMAP1      = unlist(map(umap, ~.$layout[, 1])),
                  UMAP2      = unlist(map(umap, ~.$layout[, 2])))

filter(umapTib, metric == "euclidean", n_epochs == 200) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  facet_grid(n_neighbors ~ min_dist) +
  geom_point() +
  theme_bw()
```

```{r}
filter(umapTib, n_neighbors == 15, min_dist == 0.1) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  facet_grid(metric ~ n_epochs) +
  geom_point() +
  theme_bw()
```

```{r}
# FINAL UMAP ----
swissUmap <- select(swissTib, -Status) %>%
  as.matrix() %>%
  umap(n_neighbors = 7, min_dist = 0.1,
       metric = "manhattan", n_epochs = 200, verbose = TRUE)

swissTibUmap <- swissTib %>%
  mutate_if(.funs = scale, .predicate = is.numeric, scale = FALSE) %>% # center variables
  mutate(UMAP1 = swissUmap$layout[, 1], UMAP2 = swissUmap$layout[, 2]) %>%
  gather(key = "Variable", value = "Value", c(-UMAP1, -UMAP2, -Status))

ggplot(swissTibUmap, aes(UMAP1, UMAP2, col = Value, shape = Status)) +
  facet_wrap(~ Variable) +
  geom_point(size = 3) +
  scale_color_gradient(low = "dark blue", high = "cyan") +
  theme_bw()
```





























