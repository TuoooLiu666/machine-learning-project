---
title: "proj3_svm_applications"
author: "Tuo Liu"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(e1071)
```

# Support Vector Machines

We use the `e1071` library in `R` to demonstrate the support vector classifier and the SVM. 

## Support Vector Classifier
The `e1071` library contains implementations for a number of statistical learning methods. In particular, the `svm()` function can be used to fit a support vector classifier when the argument `kernel = "linear"` is used. A `cost` argument allows us to specify the cost of a violation to the margin.  When the `cost` argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin.  When the `cost` argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.

The implementation for linear SVM minimizes following loss function which is equivalent to the optimization problem given in ISLP: 
$$
\min_{\beta_0. \beta}\sum_{i=1}^{n}[1-y_if(x_i)]_++\lambda||\beta||^2
$$
where $[1-y_if(x_i)]_+$ is called *hinge* loss.  

## Support Vector Machine

In order to fit an SVM using a non-linear kernel, we  use the `svm()` function. However, now we use a different value of the parameter `kernel`. To fit an SVM with a polynomial kernel we use `kernel = "polynomial"`, and to fit an SVM with a radial kernel we use `kernel = "radial"`.
In the former case we also use the `degree` argument to specify a degree for the polynomial kernel, and in the latter case we use `gamma` to specify a value of $\gamma$ for the radial basis kernel.

```{r}
set.seed(1)
x <- matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] - 2
y <- c(rep(1, 150), rep(2, 50))
dat <- data.frame(x = x, y = as.factor(y))

# plot data points
plot(x, col = y)
```

```{r chunk18}
set.seed(1)
train <- sample(200, 100)
svmfit <- svm(y ~ ., data = dat[train, ], 
              kernel = "radial",  gamma = 1, cost = 1)
plot(svmfit, dat[train, ])
```

The plot shows that the resulting  SVM has a decidedly non-linear  boundary. We can perform cross-validation using `tune()` to select the best choice of $\gamma$ and `cost` for an SVM with a radial kernel:

```{r}
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat[train, ], 
    kernel = "radial", 
    ranges = list(
      cost = c(0.1, 1, 10, 100, 1000),
      gamma = c(0.5, 1, 2, 3, 4)
    )
  )
summary(tune.out)
```

## Application to Gene Expression Data


## ROC Curves