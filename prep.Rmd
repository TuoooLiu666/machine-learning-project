---
title: "prep"
author: "LT"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(mlbench,e1071,Amelia)
```

## you must understand your data

Better understanding equals better results! Try use descriptive statistics and visualization!

- data cleaning
- data transformation/feature engineering
- modeling choice

### summary
```{r}
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
dim(PimaIndiansDiabetes)
sapply(PimaIndiansDiabetes, class)

# class distribution
PimaIndiansDiabetes$diabetes |> table() |> prop.table()

# standard deviation
sapply(PimaIndiansDiabetes[,1:8], sd)

# skewness
sapply(PimaIndiansDiabetes[,1:8], skewness)

# correlation
cor(PimaIndiansDiabetes[,1:8], method = 'p')
```
### visualization
- univariate viz


```{r}
# load the data
data(iris)
# create histograms for each attribute
par(mfrow=c(1,4))
for(i in 1:4) {
  hist(iris[,i], main=names(iris)[i])
}

# density plot
par(mfrow=c(1,4))
for (i in 1:4)
  plot(density(iris[,i]), main = names(iris)[i])
```
```{r}
# boxplot
par(mfrow=c(1,4))
for(i in 1:4) {
  boxplot(iris[,i], main=names(iris)[i])}
```

```{r}
# missing plot
data(Soybean)
Amelia::missmap(Soybean, col = c("black", "white"), legend = T)
```

- multivariate viz
```{r}
require(corrplot)

data("iris")
correlations <- cor(iris[,1:4])
corrplot(correlations, method = 'pie')
```
```{r}
# pair-wise scatterplots 
pairs(iris[,1:4])
```

```{r}
pairs(Species ~., data = iris, col = iris$Species)
```

```{r}
require(caret)
# density/box plots for each attribute by class value
x <- iris[,1:4]
y <- iris[,5]

scales <- list(x=list(relation="free"), y=list(relation="free"))
caret::featurePlot(x=x, y=y, plot = 'box', scales=scales)
```


## DATA PREPROCESSING

- basic transformation: centering, scaling, standardization and normalization
- power transformation like Box-Cox and Yeo-Johnson
- many-to-many transformation, PCA/ICA

Why data pre-processing?

some machine learning algorithms require the data to be in a specific form, whereas other algorithms can perform better if the data is prepared in a specific way.

### which preprocessing method to use?
It's hard to know which data preprocessing methods to use. The rule of thumb is

- instance based methods are more effective if the input attributes have the same scale.
regression methods can work better if the input attributes are standardized.


## ACTION with caret

`caret` transformations can be used in two ways

- standalone: transformations can be modeled from training data and applied to multiple datasets. The model of the transform is prepared using the `preProcess()` function and applied to a dataset using the `predict()` function.
- training: transformations can be modeled from training data and applied automatically during model evaluation. transformations applied during training are prepared using the `preProcess()` function and passed to the `train()` function via the preProcess argument.


transformation methods  

- BoxCox: values must be non-zero and positive
- YeoJohnson: values can be negative
- expoTrans: power transformation 
- zv: remove attributes with a zero variance
- nzv: remove attibutes with a near zero variance
- center: divide values by standard deviation
- scale: subtrat mean from values
- range: normalize values
- pca: transform data to the principle components
- ica: transform data to independent components
- spatiaSign: project data onto a unit circle


```{r}
##############################################################
# scale transform: calculate sd for an feature and divide each value by that sd
# center
# standardize = center + scale to have features with mean 0 and standard deviation 1.
##############################################################
require(caret)
# scale
data("iris")
# summarize data
summary(iris[,1:4])
# calculate the pre-process parameter from the dataset
preprocessParams <- preProcess(iris[,1:4], method = c("scale", "center"))
# apply preprocess parameters to the dataset
transformed <- predict(preprocessParams, iris[,1:4])
# summarize scaled results
summary(transformed)
```
```{r}
##############################################################
# normalization: scale feature to have range of [0,1]
##############################################################
# summarize data
summary(iris[,1:4])
# calculate the pre-process parameters from the dataset
preprocessParams <- preProcess(iris[,1:4], method=c("range"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, iris[,1:4])
# summarize the transformed dataset
summary(transformed)
```
```{r}
require(caret)
require(mlbench)
require(e1071)
##############################################################
# boxcox: when a feature has a Guassian-like distribution but is skewed, boxcox can shift it to reduce skewness and make it more Gaussian.
##############################################################
data("PimaIndiansDiabetes")
# summarize data
apply(PimaIndiansDiabetes[,7:8], 2, e1071::skewness)
boxplot(PimaIndiansDiabetes[,7:8])
# calculate the pre-process parameters from the dataset
preprocessParams <- preProcess(PimaIndiansDiabetes[,7:8], method=c("BoxCox"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, PimaIndiansDiabetes[,7:8])
# summarize the transformed dataset (note pedigree and age)
apply(transformed, 2, e1071::skewness)
boxplot(transformed)
```

```{r}
##############################################################
# YeoJohnson: like boxcox, but it supports values that are equal to 0 and negative
##############################################################
# summarize data
apply(PimaIndiansDiabetes[,7:8], 2, e1071::skewness)
boxplot(PimaIndiansDiabetes[,7:8])
# calculate the pre-process parameters from the dataset
preprocessParams <- preProcess(PimaIndiansDiabetes[,7:8], method=c("YeoJohnson"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, PimaIndiansDiabetes[,7:8])
# summarize the transformed dataset (note pedigree and age)
apply(transformed, 2, e1071::skewness)
boxplot(transformed)
```

```{r}
##############################################################
# pca: the result is attributes that are uncorrelated, useful for algorithms like linear and generalized linear regression
##############################################################
require(mlbench)
summary(iris)
preprocessParams <- preProcess(iris, method = c('center', 'scale', 'pca'))
print(preprocessParams)
transformed <- predict(preprocessParams, iris)
summary(transformed)
```
```{r}
##############################################################
# ica: the result is independent components, unlike PCA, ICA retains those components that are independent, thus you must specify desired independent components with the n.comp argument. this transformation may be useful for algorithms such as Naive Bayes.
##############################################################
require(caret)
require(mlbench)
# summarize dataset
summary(PimaIndiansDiabetes[,1:8])
# calculate the pre-process parameters from the dataset
preprocessParams <- preProcess(PimaIndiansDiabetes[,1:8], method=c("center", "scale",
"ica"), n.comp=5)
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, PimaIndiansDiabetes[,1:8])
# summarize the transformed dataset
summary(transformed)
```
## RESAMPLING METHODS TO ESTIMATE MODEL ACCURACY

- how to split data into training and assessment set?
- how to evaluate model accuracy using bootstrap method?
- how to evaluate model accuracy using k-fold cross-validation with and without repeats
- how to evaluate model accuracy using LOOCV?

```{r}
require(caret)
require(klaR)

# data split: define an 80/20 split 
trainIndex <- createDataPartition(iris$Species, 
                                  p = .8, 
                                  list = F)
# split
dataTrain <- iris[trainIndex,]
dataTest <- iris[-trainIndex,]
# train model
fit <- NaiveBayes(Species ~ ., data = dataTrain)
# predict
predictions <- predict(fit, dataTest[,1:4])
# summary
confusionMatrix(predictions$class, dataTest$Species)
```
```{r}
require(caret)
# bootstrap: bootstrap resampling takes random samples from the dataset against which the model is evaluated. in aggregate, the results provide an indication of the variance of the model performance.

# define training control
trainControl <- trainControl(method = 'boot', number = 100)
# train model
fit <- train(Species ~ ., 
             data = iris, 
             trControl = trainControl,
             method = 'nb') # specify model
# summary
fit
```

```{r}
# k-fold CV/LOOCV: involves splitting the dataset into k-subsets. Each subset is held out while the model is trained on all other subsets.

# define training control
trainControl1 <- trainControl(method = 'cv', number = 10) # without repeat
trainControl2 <- trainControl(method = 'cv', number = 10, repeats = 5) # with repeats
trainControl3 <- trainControl(method = 'LOOCV')

# train model
fit1 <- train(Species ~ ., 
             data = iris, 
             trControl = trainControl1,
             method = 'nb')
fit2 <- train(Species ~ ., 
             data = iris, 
             trControl = trainControl2,
             method = 'nb')
fit3 <- train(Species ~ ., 
             data = iris, 
             trControl = trainControl3,
             method = 'nb')
# summary
fit1
fit2
fit3
```


## MODEL EVALUATION METRICS

- how to use accuracy and Kappa on classification problems?
- how to use $RMSE$ and $R^2$ on regression problems?
- how to use Area under ROC curve, sensitivity and specificity on binary classification problems?
- how to use Logarithmic Loss to evaluate classifiers?


### ACCURACY & KAPPA
the default metrics for alforithms on binary and multi-class classification data sets in `caret`.

- accuracy is useful for binary classifier
- kappa is useful when having imbalanced classes
```{r}
# load packages
require(caret)
require(mlbench)
# load the dataset
data(PimaIndiansDiabetes)
# prepare resampling method
trainControl <- trainControl(method="cv", 
                             number=5)

set.seed(7)
fit <- train(diabetes~., 
             data=PimaIndiansDiabetes, 
             method="glm", 
             metric="Accuracy",
             trControl=trainControl)
# display results
print(fit)
```

```{r}
# load data
data(longley)
# prepare resampling method
trainControl <- trainControl(method="cv", 
                             number=5)
set.seed(7)
fit <- train(Employed~., 
             data=longley, 
             method="lm", 
             metric="RMSE", 
             trControl=trainControl)
# display results
print(fit)

```

## WHAT ALGORITHM SHOULD YOU USE ON YOUR DATA?

- HOW TO MODEL DATA WITH LINER AND NON-LINEAR ML ALGORITHMS?
- HOW TO SPOT-CHECK A SUITE OF LINEA AND NON-LINEAR ALGORITHMS?
- WHICH LINEAR AND NON-LINEA ALGORITHMS TO USE?

### LINEAR ALGORITHMS

- linear regression
- logistic regression
- linear discriminant analysis
- regularized regression

```{r}
# prepare data
require(caret)
require(mlbench)
data("PimaIndiansDiabetes")
# define train parameters
trainControl <- trainControl(method = "repeatedcv", 
                             number = 10,
                             repeats = 3)
# train liner models
# LDA
set.seed(7)
fit.lda <- train(diabetes~., 
                 data=PimaIndiansDiabetes, 
                 method="lda", 
                 trControl=trainControl)
```

### NON-LINEAR ALGORITHMS

- k-Nearest-Neighbors
- Naive Bayes
- Support Vector Machine
- classification and Regression Trees

```{r}
# SVM
set.seed(7)
fit.svm <- train(diabetes~., 
                 data=PimaIndiansDiabetes, 
                 method="svmRadial",
                 trControl=trainControl)

# KNN
set.seed(7)
fit.knn <- train(diabetes~., 
                 data=PimaIndiansDiabetes, 
                 method="knn", 
                 trControl=trainControl)
# CART
set.seed(7)
fit.cart <- train(diabetes~., 
                  data=PimaIndiansDiabetes, 
                  method="rpart",
                  trControl=trainControl)

# Random Forest
set.seed(7)
fit.rf <- train(diabetes~., 
                data=PimaIndiansDiabetes, 
                method="rf", 
                trControl=trainControl)
```

### COMPARE ALGORITHM PERFORMANCE
```{r}
# collect resamples
results <- resamples(list(CART=fit.cart, 
                          LDA=fit.lda, 
                          SVM=fit.svm, 
                          KNN=fit.knn, 
                          RF=fit.rf))

# summarize differences between modes
summary(results)

# box and whisker plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)
dotplot(results, scales=scales)
```

## HYPERPARAMETER TUNING

- HOW TO PERFORM A GRID SEARCH OR RANDOM SEARCH?
- HOW TO USE TOOLS THAT COME WITH ALGORITHMS TO TUNE PARAMETERS?
- HOW TO EXTEND ALGORITHM TUNING?

### TEST ALGORITHM
```{r}
# set up
require(randomForest)
require(caret)
require(mlbench)

# Load Dataset
data(Sonar)
dataset <- Sonar
x <- dataset[,1:60]
y <- dataset[,61]

# Create model with default paramters
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3)
seed <- 7
metric <- "Accuracy"

set.seed(seed)
mtry <- sqrt(ncol(x)) # default NO. OF PREDICTORS
tunegrid <- expand.grid(.mtry=mtry) # DEFINE GRID FOR SEARCH

rfDefault <- train(Class~., 
                   data=dataset, 
                   method="rf", 
                   metric=metric, 
                   tuneGrid=tunegrid,
                   trControl=trainControl)
print(rfDefault)
```
### RANDOM SEARCH
One strategy is to try random values within a range. Now s try a random search for `mtry` using the caret package:

```{r}
# Random Search
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3, 
                             search="random")

set.seed(seed)
mtry <- sqrt(ncol(x))

rfRandom <- train(Class~., 
                  data=dataset, 
                  method="rf", 
                  metric=metric, 
                  tuneLength=15,
                  trControl=trainControl)
print(rfRandom)
plot(rfRandom)
```
### GRID SEARCH

ANOTHER search strategy would be to define a grid of values for the algortihm parameter to try. each axis of the grid is an algorithm parameter, and points in the grid are specific combinations of parameters. Next comes a grid example where we only tune one parameter so the grid search is a linear search through a vector of candidate values:
```{r}
# GRID SEARCH
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3, 
                             search="grid")
# define grid
set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:15))

# train
rfGrid <- train(Class~., 
                data=dataset, 
                method="rf", 
                metric=metric, 
                tuneGrid=tunegrid,
                trControl=trainControl)

print(rfGrid)
plot(rfGrid)
```
### Tune using built-in tools

some algorithm implementations provide tools for parameter tuning, 

- random forest
```{r}
# Algorithm Tune (tuneRF)
set.seed(seed)
bestmtry <- tuneRF(x, y, 
                   stepFactor=1.5, 
                   improve=1e-5, 
                   ntree=500)
print(bestmtry)
```


### craft your own parameter search
- tune manually

One approach is to create many models for our algorithm and pass a vector of candidate values directly to the algorithm manually.
```{r}
# manual search
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3, 
                             search="grid")

tunegrid <- expand.grid(.mtry = c(sqrt(ncol(x))))
modellist <- list()

for (ntree in c(1000, 1500, 2000, 2500)) {
  set.seed(seed)
  fit <- train(Class ~ ., data = dataset,
               method = "rf",
               metric = "metric",
               tuneGrid = tunegrid,
               trControl = trainControl,
               ntree = ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}


# compare models
results <- resamples(modellist)
summary(results)
dotplot(results)
```


- extend caret

another approach is to create a new appraoch for caret: the is the same random forest algorithm, only modified so that it supprots tuning of multiple parameters:
```{r}
customRF <- list(type="Classification", library="randomForest", loop=NULL)
customRF$parameters <- data.frame(parameter=c("mtry", "ntree"), class=rep("numeric", 2),
label=c("mtry", "ntree"))
customRF$grid <- function(x, y, len=NULL, search="grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry=param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc=NULL, submodels=NULL)
  predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc=NULL, submodels=NULL)
  predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
```

```{r}
# train model
trainControl <- trainControl(method="repeatedcv", 
                             number=10, 
                             repeats=3)
tunegrid <- expand.grid(.mtry=c(1:15), 
                        .ntree=c(1000, 1500, 2000, 2500))

set.seed(seed)
custom <- train(Class~., 
                data=dataset, 
                method=customRF, # use custom algorithm
                metric=metric, 
                tuneGrid=tunegrid,
                trControl=trainControl)

summary(custom)
plot(custom)
```



## ENSEMBLE
ENSEMBLES CAN PRODUCE A BOOST IN ACCURACY AS THEY IMPROVE RESULTS BY COMBINING **PREDICTIONS** FROM MULTIPLE MODELS. 

- HOW TO USE **BOOSTING** AND **BAGGING** ALGORITHMS?
- HOW TO BLEND THE PREDICTIONS FROM MULTIPLE MODELS USING **STACKING**?


Once WE have a shortlist of accurate models, WE can use PARAMETER tuning to get the most from each MODEL. another approach used to increase accuracy is to combine the predictions of multiple models together and get an *ensemble prediction*. popular methods for combining predictions from different models are:

- *Bagging*:  building multiple models (of the same type) from different sub-samples of the training data sets.
- *Boosting*: building multiple models (of the same type) each of which learns to fix the prediction errors of a prior model in the chain.
- *Stacking*: building multiple models (of different types) and supervisor model that learns how to best combine the predictions of the primary models.

```{r}
require(mlbench)
require(caret)
require(caretEnsemble)

# Load the dataset
data(Ionosphere)
dataset <- Ionosphere
dataset <- dataset[,-2]
dataset$V1 <- as.numeric(as.character(dataset$V1))

dataset |> colnames()
```

### BOOSTING ALGORITHMS
- C5.0
- Stochastic Gradient Boosting

```{r}
# Example of Boosting Algorithms
trainControl <- trainControl(method="repeatedcv", 
                             number=10, repeats=3)
seed <- 7
metric <- "Accuracy"

# C5.0
set.seed(seed)
fit.c50 <- train(Class~., data=dataset, 
                 method="C5.0", metric=metric,
                 trControl=trainControl)


# Stochastic Gradient Boosting
set.seed(seed)
fit.gbm <- train(Class~., data=dataset, 
                 method="gbm", metric=metric,
                 trControl=trainControl, 
                 verbose=FALSE)


# summarize results
boostingResults <- resamples(list(c5.0=fit.c50, gbm=fit.gbm))
summary(boostingResults)
dotplot(boostingResults)
```

### BAGGING
- bagged classification and regression trees (CART)
- random forest

examples shown above have parameters that are not tuned here.
```{r}
# Example of Bagging algorithms
trainControl <- trainControl(method="repeatedcv", 
                             number=10, repeats=3)
seed <- 7
metric <- "Accuracy"
# Bagged CART
set.seed(seed)
fit.treebag <- train(Class~., data=dataset, 
                     method="treebag", metric=metric,
                     trControl=trainControl)
# Random Forest
set.seed(seed)
fit.rf <- train(Class~., data=dataset, 
                method="rf", metric=metric, 
                trControl=trainControl)
# summarize results
baggingResults <- resamples(list(treebag=fit.treebag, rf=fit.rf))
summary(baggingResults)
dotplot(baggingResults)
```


### STACKING ALGORITHMS
given ia list of caret models and the `caretStack()` function can combine predictions of multiple models as implemented in the caretEnsemble package. 

Below is an example that creates these 5 sub-models.
- LDA
- CART
- KNN
- SVM with a radial basis kernel function

```{r}
# Example of Stacking algorithms
# create submodels
trainControl <- trainControl(method="repeatedcv", 
                             number=10, repeats=3,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

algorithmList <- c('lda', 'rpart', 'glm', 'knn', 'svmRadial')

set.seed(seed)
models <- caretEnsemble::caretList(Class~., data=dataset, 
                    trControl=trainControl, 
                    methodList=algorithmList)
# summary
results <- resamples(models)
summary(results)
dotplot(results)

# correlation between results
modelCor(results) # .75
splom(results)
```

- combine predictions of the classifiers using a simple linear model
```{r}
# stack using glm
stackControl <- trainControl(method="repeatedcv", 
                             number=10, repeats=3,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

set.seed(seed)
stack.glm <- caretEnsemble::caretStack(models, method="glm", 
                        metric="Accuracy", 
                        trControl=stackControl)

print(stack.glm)
```


- combine predictions of the classifiers using random forest algorithm
```{r}
# stack using random forest
set.seed(seed)
stack.rf <- caretEnsemble::caretStack(models, method="rf", 
                        metric="Accuracy", 
                        trControl=stackControl)

print(stack.rf)
```

## Finalize and SAVE ML MODEL

- how to make prediction on new data?
- how to recreate a well performing model from caret as a standalone model?
- how to save model to a file, load it and make prediction on unseen data?


### predictions
```{r}
# load packages
require(caret)
require(mlbench)
# load dataset
data(PimaIndiansDiabetes)
# create 80%/20% for training and validation set
set.seed(9)
validationIndex <- createDataPartition(PimaIndiansDiabetes$diabetes, p=0.80, list=FALSE)
validation <- PimaIndiansDiabetes[-validationIndex,]
training <- PimaIndiansDiabetes[validationIndex,]
# train a model and summarize model
set.seed(9)
trainControl <- trainControl(method="cv", number=10) # 10-fold CV without repeat
fit.lda <- train(diabetes~., data=training, 
                 method="lda", metric="Accuracy",
                 trControl=trainControl)
print(fit.lda)
print(fit.lda$finalModel)
# estimate skill on validation set
set.seed(9)
predictions <- predict(fit.lda, newdata=validation)
confusionMatrix(predictions, validation$diabetes)
```
### creating a standalone model using all training data
```{r}
require(randomForest)
# load dataset
data(Sonar)
set.seed(7)
# create 80%/20% for training and validation datasets
validationIndex <- createDataPartition(Sonar$Class, p=0.80, list=FALSE)
validation <- Sonar[-validationIndex,]
training <- Sonar[validationIndex,]
# train a model and summarize model
set.seed(7)
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
fit.rf <- train(Class~., data=training, 
                method="rf", metric="Accuracy",
                trControl=trainControl, ntree=2000)
print(fit.rf)
print(fit.rf$finalModel)

# create standalone model using all training data
set.seed(7)
finalModel <- randomForest(Class~., data = training, 
                           mtry=2, ntree=2000) # from final model summary
# make a predictions on "new data" using the final model
finalPredictions <- predict(finalModel, validation[,1:60])
confusionMatrix(finalPredictions, validation$Class)
```



### saving the model to file for later loading and making predictions

save the best model to a file so that it can be loaded up later and make predictions. We train the final model using the training set and our optimal parameters, then save it to a file called finalModel.rds in the local working directory.

```{r}
# load dataset
data(Sonar)
set.seed(7)
# create 80%/20% for training and validation datasets
validationIndex <- createDataPartition(Sonar$Class, p=0.80, list=FALSE)
validation <- Sonar[-validationIndex,]
training <- Sonar[validationIndex,]
# create final standalone model using all training data
set.seed(7)
finalModel <- randomForest(Class~., training, mtry=2, ntree=2000)
# save the model to disk
saveRDS(finalModel, "./finalModel.rds")

# later...
# load the model
superModel <- readRDS("./finalModel.rds")
print(superModel)
# make a predictions on "new data" using the final model
finalPredictions <- predict(superModel, validation[,1:60])
confusionMatrix(finalPredictions, validation$Class)

```








































































